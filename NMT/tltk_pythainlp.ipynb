{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/tltk/\n",
    "# !pip install tltk\n",
    "# !pip install tltk-mtl -> 도커 사용필요.\n",
    "# Description TLTK is a tool for computing Metric Temporal logic robustness. \n",
    "# This is done by specifing predicates in the form Ax <= b and using those in MTL formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.dict_vectorizer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction. Anything that cannot be imported from sklearn.feature_extraction is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DictVectorizer from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/iris/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.19.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<NEo>สำนักงาน/NOUN|เขต/NOUN|จตุจักร/PROPN|</NEo>ชี้แจง/VERB|ว่า/SCONJ|<s/>/PUNCT|ได้/AUX|นำ/VERB|ป้ายประกาศ/NOUN|เตือน/VERB|ปลิง/NOUN|ไป/VERB|ปัก/VERB|ตาม/ADP|แหล่งน้ำ/NOUN|<u/>ใน/ADP|<NEl>เขต/NOUN|อำเภอ/NOUN|เมือง/NOUN|<s/>/PUNCT|จังหวัด/NOUN|อ่างทอง/PROPN|</NEl><u/>หลังจาก/SCONJ|<NEp>นาย/NOUN|สุ/PROPN|กิจ/NOUN|</NEp><s/>/PUNCT|อายุ/NOUN|<u/>65/NUM|<s/>/PUNCT|ปี/NOUN|<u/>ถูก/AUX|ปลิง/VERB|กัด/VERB|แล้ว/ADV|ไม่ได้/AUX|ไป/VERB|พบ/VERB|แพทย์/NOUN|<u/>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk parsing. The output includes markups for word segments (|), \n",
    "# elementary discourse units (<u/>), pos tags (/POS),and named entities (<NEx>…</NEx>)\n",
    "tltk.nlp.chunk('สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ '\\\n",
    "               'ในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<NEo>สำนักงานเขตจตุจักร</NEo>ชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ ใน<NEl>เขตอำเภอเมือง จังหวัดอ่างทอง</NEl> หลังจาก<NEp>นายสุกิจ</NEp> อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์ '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output includes markups for named entities (<NEx>…</NEx>)\n",
    "tltk.nlp.ner_tag('สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ '\\\n",
    "                 'ในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('สำนักงาน', 'NOUN', 'B-O'),\n",
       " ('เขต', 'NOUN', 'I-O'),\n",
       " ('จตุจักร', 'PROPN', 'I-O'),\n",
       " ('ชี้แจง', 'VERB', 'O'),\n",
       " ('ว่า', 'SCONJ', 'O'),\n",
       " ('<s/>', 'PUNCT', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module for named entity recognition (person, organization, location)\n",
    "tltk.nlp.ner([('สำนักงาน', 'NOUN'), ('เขต', 'NOUN'), ('จตุจักร', 'PROPN'), ('ชี้แจง', 'VERB'), \\\n",
    "              ('ว่า', 'SCONJ'), ('<s/>', 'PUNCT')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('โปรแกรม', 'NOUN'),\n",
       "  ('สำหรับ', 'ADP'),\n",
       "  ('ใส่', 'VERB'),\n",
       "  ('แท็ก', 'NOUN'),\n",
       "  ('หมวดคำ', 'NOUN'),\n",
       "  ('ภาษาไทย', 'PROPN'),\n",
       "  ('<s/>', 'PUNCT')],\n",
       " [('วันนี้', 'NOUN'),\n",
       "  ('ใช้งาน', 'VERB'),\n",
       "  ('ได้', 'ADV'),\n",
       "  ('บ้าง', 'ADV'),\n",
       "  ('แล้ว', 'ADV'),\n",
       "  ('<s/>', 'PUNCT')]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tltk.nlp.pos_tag('โปรแกรมสำหรับใส่แท็กหมวดคำภาษาไทย วันนี้ใช้งานได้บ้างแล้ว')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('โ', 'NOUN'),\n",
       " ('ป', 'NOUN'),\n",
       " ('ร', 'VERB'),\n",
       " ('แ', 'NOUN'),\n",
       " ('ก', 'NOUN'),\n",
       " ('ร', 'VERB'),\n",
       " ('ม', 'NOUN'),\n",
       " ('ส', 'NOUN'),\n",
       " ('ำ', 'NOUN'),\n",
       " ('ห', 'NOUN'),\n",
       " ('ร', 'VERB'),\n",
       " ('ั', 'NOUN'),\n",
       " ('บ', 'NOUN'),\n",
       " ('ใ', 'VERB'),\n",
       " ('ส', 'NOUN'),\n",
       " ('่', 'NOUN'),\n",
       " ('แ', 'NOUN'),\n",
       " ('ท', 'NOUN'),\n",
       " ('็', 'NOUN'),\n",
       " ('ก', 'NOUN'),\n",
       " ('ห', 'NOUN'),\n",
       " ('ม', 'NOUN'),\n",
       " ('ว', 'NOUN'),\n",
       " ('ด', 'NOUN'),\n",
       " ('ค', 'NOUN'),\n",
       " ('ำ', 'NOUN'),\n",
       " ('ภ', 'NOUN'),\n",
       " ('า', 'NOUN'),\n",
       " ('ษ', 'NOUN'),\n",
       " ('า', 'NOUN'),\n",
       " ('ไ', 'NOUN'),\n",
       " ('ท', 'NOUN'),\n",
       " ('ย', 'VERB'),\n",
       " (' ', 'NOUN'),\n",
       " ('ว', 'NOUN'),\n",
       " ('ั', 'NOUN'),\n",
       " ('น', 'NOUN'),\n",
       " ('น', 'NOUN'),\n",
       " ('ี', 'NOUN'),\n",
       " ('้', 'NOUN'),\n",
       " ('ใ', 'VERB'),\n",
       " ('ช', 'NOUN'),\n",
       " ('้', 'NOUN'),\n",
       " ('ง', 'NOUN'),\n",
       " ('า', 'NOUN'),\n",
       " ('น', 'NOUN'),\n",
       " ('ไ', 'NOUN'),\n",
       " ('ด', 'NOUN'),\n",
       " ('้', 'NOUN'),\n",
       " ('บ', 'NOUN'),\n",
       " ('้', 'NOUN'),\n",
       " ('า', 'NOUN'),\n",
       " ('ง', 'NOUN'),\n",
       " ('แ', 'NOUN'),\n",
       " ('ล', 'NOUN'),\n",
       " ('้', 'NOUN'),\n",
       " ('ว', 'NOUN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as \"tltk.nlp.pos_tag\", but the input is a word list, [w1,w2,…]\n",
    "tltk.nlp.pos_tag_wordlist('โปรแกรมสำหรับใส่แท็กหมวดคำภาษาไทย วันนี้ใช้งานได้บ้างแล้ว')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'แต่|อาจ|เพราะ|นกกินปลีอกเหลือง|เป็น|พ่อแม่|มือใหม่|<s/>|รัง|ที่|ทำ|จึง|ไม่ค่อย|แข็งแรง<u/>วัน|หนึ่ง|รัง|ก็|ฉีก|เกือบ|ขาด|เป็น|สอง|ท่อน|ห้อย|ต่องแต่ง<u/>ผม|พยายาม|หา|อุปกรณ์|มา|ยึด|รัง|กลับคืน|รูปทรง|เดิม<u/>ขณะ|ที่|แม่|นกกินปลีอกเหลือง|ส่งเสียง|โวยวาย|อยู่|ใกล้|ๆ|<s/>|<u/>แต่|สุดท้าย|ไม่|สำเร็จ<u/>สอง|สาม|วัน|ต่อ|มา|รัง|ที่|ช่วย|ซ่อม|ก็|พัง|ไป<u/>ไม่|เห็น|แม่|นก|บิน|กลับ|มา|อีก|เลย<u/>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segment a paragraph into elementary discourse units (edu) marked with <u/> and segment words in each edu\n",
    "tltk.nlp.segment('แต่อาจเพราะนกกินปลีอกเหลืองเป็นพ่อแม่มือใหม่ รังที่ทำจึงไม่ค่อยแข็งแรง '\\\n",
    "                'วันหนึ่งรังก็ฉีกเกือบขาดเป็นสองท่อนห้อยต่องแต่ง ผมพยายามหาอุปกรณ์มายึดรังกลับคืนรูปทรงเดิม '\\\n",
    "                'ขณะที่แม่นกกินปลีอกเหลืองส่งเสียงโวยวายอยู่ใกล้ ๆ แต่สุดท้ายไม่สำเร็จ สองสามวันต่อมารังที่ช่วยซ่อมก็พังไป '\\\n",
    "                'ไม่เห็นแม่นกบินกลับมาอีกเลย')\n",
    "\n",
    "\n",
    "# edu segmentation is based on syllable input using RandomForestClassifier model, \n",
    "# which is trained on an edu-segmented corpus (approx. 7,000 edus) created and used in Nalinee's thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tltk.nlp.word_segment(Text,method='mm|ngram|colloc') :\n",
    "# word segmentation using either maximum matching or ngram or maximum collocation approach. \n",
    "# 'colloc' is used by default. Please note that the first run of ngram method would\n",
    "# take a long time because TNC.3g will be loaded for ngram calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ผู้สื่อข่าว|รายงาน|ว่า|นายกรัฐมนตรี|ไม่|มา|ทำงาน|ที่|ทำเนียบรัฐบาล|<s/>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tltk.nlp.word_segment('ผู้สื่อข่าวรายงานว่านายกรัฐมนตรีไม่มาทำงานที่ทำเนียบรัฐบาล')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'โปร~แกรม~สำ~หรับ~ประ~มวล~ผล~ภา~ษา~ไทย~<s/>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syllable segmentation using 3gram statistics\n",
    "tltk.nlp.syl_segment('โปรแกรมสำหรับประมวลผลภาษาไทย')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['คนขับ|รถประจำทาง|ปรับอากาศ',\n",
       "  'คนขับรถ|ประจำทาง|ปรับอากาศ',\n",
       "  'คน|ขับ|รถประจำทาง|ปรับอากาศ',\n",
       "  'คน|ขับรถ|ประจำทาง|ปรับอากาศ',\n",
       "  'คนขับ|รถ|ประจำทาง|ปรับอากาศ',\n",
       "  'คนขับรถ|ประจำ|ทาง|ปรับอากาศ',\n",
       "  'คนขับ|รถประจำทาง|ปรับ|อากาศ',\n",
       "  'คนขับรถ|ประจำทาง|ปรับ|อากาศ',\n",
       "  'คน|ขับ|รถ|ประจำทาง|ปรับอากาศ',\n",
       "  'คนขับ|ร|ถ|ประจำทาง|ปรับอากาศ']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.word_segment_nbest(Text, N) : \n",
    "# return the best N segmentations based on the assumption of minimum word approach.\n",
    "tltk.nlp.word_segment_nbest('คนขับรถประจำทางปรับอากาศ',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"สถา~บัน~อุ~ดม~ศึก~ษา|ไม่|สา~มารถ|ก้าว|ให้|ทัน|การ|เปลี่ยน~แปลง|ของ|ตลาด~แรง~งาน<tr/>sa1'thaa4~ban0~?u1~dom0~sUk1~saa4|maj2|saa4~maat2|kaaw2|haj2|than0|kaan0|pliian1~plxxN0|khOON4|ta1'laat1~rxxN0~Naan0|<s/>\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.g2p(Text)\n",
    "# return Word segments and pronunciations \n",
    "tltk.nlp.g2p('สถาบันอุดมศึกษาไม่สามารถก้าวให้ทันการเปลี่ยนแปลงของตลาดแรงงาน')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loŋ1 mɛː3.naːm4 rᴐː1 dɤːn1 paj1 haː5 plaː1 <s/>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.th2ipa(Text) : \n",
    "# return Thai transcription in IPA forms \n",
    "tltk.nlp.th2ipa('ลงแม่น้ำรอเดินไปหาปลา')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'khue khao doen loei long pai ro nai maenam sa-at pai ha maprang <s/>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.th2roman(Text) : \n",
    "# return Thai romanization according to Royal Thai Institute guideline.\n",
    "tltk.nlp.th2roman('คือเขาเดินเลยลงไปรอในแม่น้ำสะอาดไปหามะปราง')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('รอย~กร่าง', 'rᴐːj1.ka2.raːŋ2'),\n",
       " ('รอย~กร่าง', 'rᴐːj1.kraːŋ2'),\n",
       " ('รอ~ย~กร่าง', 'rᴐː1.jᴐː1.ka2.raːŋ2'),\n",
       " ('รอ~ย~กร่าง', 'rᴐː1.jᴐː1.kraːŋ2'),\n",
       " ('ร~อ~ย~กร่าง', 'rᴐː1.ʔᴐː1.jᴐː1.ka2.raːŋ2'),\n",
       " ('ร~อ~ย~กร่าง', 'rᴐː1.ʔᴐː1.jᴐː1.kraːŋ2'),\n",
       " ('ร~อยก~ร่าง', 'rᴐː1.ʔa2.jok2.raːŋ3'),\n",
       " ('รอ~ยก~ร่าง', 'rᴐː1.jok4.raːŋ3'),\n",
       " ('ร~อ~ยก~ร่าง', 'rᴐː1.ʔᴐː1.jok4.raːŋ3'),\n",
       " ('รอย~ก~ร่าง', 'rᴐːj1.kᴐː1.raːŋ3'),\n",
       " ('รอ~ย~ก~ร่าง', 'rᴐː1.jᴐː1.kᴐː1.raːŋ3'),\n",
       " ('ร~อ~ย~ก~ร่าง', 'rᴐː1.ʔᴐː1.jᴐː1.kᴐː1.raːŋ3'),\n",
       " ('รอย~กร่า~ง', 'rᴐːj1.ka2.raː2.ŋᴐː1'),\n",
       " ('รอย~กร่า~ง', 'rᴐːj1.kraː2.ŋᴐː1'),\n",
       " ('รอ~ย~กร่า~ง', 'rᴐː1.jᴐː1.ka2.raː2.ŋᴐː1'),\n",
       " ('รอ~ย~กร่า~ง', 'rᴐː1.jᴐː1.kraː2.ŋᴐː1'),\n",
       " ('ร~อ~ย~กร่า~ง', 'rᴐː1.ʔᴐː1.jᴐː1.ka2.raː2.ŋᴐː1'),\n",
       " ('ร~อ~ย~กร่า~ง', 'rᴐː1.ʔᴐː1.jᴐː1.kraː2.ŋᴐː1'),\n",
       " ('ร~อยก~ร่า~ง', 'rᴐː1.ʔa2.jok2.raː3.ŋᴐː1'),\n",
       " ('รอ~ยก~ร่า~ง', 'rᴐː1.jok4.raː3.ŋᴐː1'),\n",
       " ('ร~อ~ยก~ร่า~ง', 'rᴐː1.ʔᴐː1.jok4.raː3.ŋᴐː1'),\n",
       " ('รอย~ก~ร่า~ง', 'rᴐːj1.kᴐː1.raː3.ŋᴐː1'),\n",
       " ('รอ~ย~ก~ร่า~ง', 'rᴐː1.jᴐː1.kᴐː1.raː3.ŋᴐː1'),\n",
       " ('ร~อ~ย~ก~ร่า~ง', 'rᴐː1.ʔᴐː1.jᴐː1.kᴐː1.raː3.ŋᴐː1')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.g2p_all(Text)\n",
    "# return all transcriptions (IPA) as a list of tuple (syllable_list, transcription). \n",
    "# Transcription is based on syllable reading rules. It could be different from th2ipa.\n",
    "tltk.nlp.g2p_all('รอยกร่าง')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['รัก', 'ทักษ', 'รักษา', 'รักษ์']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tltk.nlp.spell_candidates(Word) : \n",
    "# list of possible correct words using minimum edit distance,\n",
    "tltk.nlp.spell_candidates('รักษ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other defined functions in the package\n",
    "\n",
    "# tltk.nlp.reset_thaidict() : clear dictionary content \n",
    "# tltk.nlp.read_thaidict(DictFile) : add a new dictionary \n",
    "# tltk.nlp.read_thaidict('BEST.dict') \n",
    "# tltk.nlp.check_thaidict(Word) : check whether Word exists in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tltk.corpus : basic tools for corpus enquiry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tltk.corpus.TNC_load() by default load TNC.3g. The file can be in the working directory or TLTK package directory\n",
    "\n",
    "tltk.corpus.trigram_load(TRIGRAM) ### load Trigram data from other sourse saved in tab delimited format \"W1tW2tW3tFreq\" \n",
    "e.g. tltk.corpus.load3gram('TNC.3g') 'TNC.3g' can be downloaded separately from Thai National Corpus Project.\n",
    "\n",
    "tltk.corpus.unigram(w1) return normalized frequecy (frequency/million) of w1 from the corpus\n",
    "\n",
    "tltk.corpus.bigram(w1,w2) return frequency/million of Bigram w1-w2 from the corpus \n",
    "e.g. tltk.corpus.bigram(\"หาย\",\"ดี\") => 2.331959592765809\n",
    "\n",
    "tltk.corpus.trigram(w1,w2,w3) return frequency/million of Trigram w1-w2-w3 from the corpus\n",
    "\n",
    "tltk.corpus.collocates(w, stat=\"chi2\", direct=\"both\", span=2, limit=10, minfq=1) ###return all collocates of w, STAT = {freq,mi,chi2} DIR={left,right,both} SPAN={1,2} The output is a list of tuples ((w1,w2), stat). \n",
    "e.g. tltk.corpus.collocates(\"วิ่ง\",limit=5) => [(('วิ่ง', 'แจ้น'), 86633.93952758134), (('วิ่ง', 'ตื๋อ'), 77175.29122642518), (('วิ่ง', 'กระหืดกระหอบ'), 48598.79465339733), (('วิ่ง', 'ปรู๊ด'), 41111.63720974819), (('ลู่', 'วิ่ง'), 33990.56839021914)]\n",
    "\n",
    "tltk.corpus.w2v_load() by deafult load word2vec file \"TNCc5model.bin\". The file can be in the working directory or TLTK package directory\n",
    "\n",
    "tltk.corpus.w2v_exist(w) check whether w has a vector representation e.g. tltk.corpus.w2v_exist(\"อาหาร\") => True\n",
    "\n",
    "tltk.corpus.w2v(w) return vector representation of w\n",
    "\n",
    "tltk.corpus.similarity(w1,w2) \n",
    "e.g. tltk.corpus.similarity(\"อาหาร\",\"อาหารว่าง\") => 0.783551877546\n",
    "\n",
    "tltk.corpus.similar_words(w, n=10, cutoff=0., score=\"n\") \n",
    "e.g. tltk.corpus.similar_words(\"อาหาร\",n=5, score=\"y\")\n",
    "=> [('อาหารว่าง', 0.7835519313812256), ('ของว่าง', 0.7366500496864319), ('ของหวาน', 0.703102707862854), ('เนื้อสัตว์', 0.6960341930389404), ('ผลไม้', 0.6641997694969177)]\n",
    "\n",
    "tltk.corpus.outofgroup([w1,w2,w3,…]) \n",
    "e.g. tltk.corpus.outofgroup([\"น้ำ\",\"อาหาร\",\"ข้าว\",\"รถยนต์\",\"ผัก\"]) => \"รถยนต์\"\n",
    "\n",
    "tltk.corpus.analogy(w1,w2,w3,n=1) \n",
    "e.g. tltk.corpus.analogy('ผู้ชาย','พ่อ','แม่') => ['ผู้หญิง'] ผู้ชาย - พ่อ + แม่ = ผู้หญิง\n",
    "\n",
    "tltk.corpus.w2v_plot([w1,w2,w3,…]) => plot a scratter graph of w1-wn in two dimensions\n",
    "\n",
    "tltk.corpus.w2v_compare_color([w1,w2,w3,…]) => visualize the components of vectors w1-wn in color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Word segmentation is based on a maximum collocation approach described in this publication: \"Aroonmanakun, W. 2002. Collocation and Thai Word Segmentation. In Thanaruk Theeramunkong and Virach Sornlertlamvanich, eds. Proceedings of the Fifth Symposium on Natural Language Processing & The Fifth Oriental COCOSDA Workshop. Pathumthani: Sirindhorn International Institute of Technology. 68-75.\" (http://pioneer.chula.ac.th/~awirote/ling/SNLP2002-0051c.pdf)\n",
    "Use tltk.nlp.word_segment(Text) or tltk.nlp.syl_segment(Text) for segmenting Thai texts. Syllable segmentation now is based on a trigram model trainned on 3.1 million syllable corpus. Input text is a paragraph of Thai texts which can be mixed with English texts. Spaces in the paragraph will be marked as \"<s/>\". Word boundary is marked by \"|\". Syllable boundary is marked by \"~\". Syllables here are written syllables. One written syllable may be pronounced as two syllables, i.e. \"สกัด\" is segemnted here as one written syllable, but it is pronounced as two syllables \"sa1-kat1\".\n",
    "Determining words in a sentence is based on the dictionary and maximum collocation strength between syllables. Since many compounds and idioms, e.g. 'เตาไมโครเวฟ', 'ไฟฟ้ากระแสสลับ', 'ปีงบประมาณ', 'อุโมงค์ใต้ดิน', 'อาหารจานด่วน', 'ปูนขาวผสมพิเศษ', 'เต้นแร้งเต้นกา' etc., are included in the standard dictionary, these will likely be segmented as one word. For applications that prefer shortest meaningful words (i.e. 'รถ|โดยสาร', 'คน|ใช้', 'กลาง|คืน', 'ต้น|ไม้' as segmented in BEST corpus), users should reset the default dictionary used in this package and reload a new dictionary containing only simple words or shortest meaningful words. Use \"reset_thaidict()\" to clear default dictionary content, and \"read_thaidict('DICT_FIILE')\" to load a new dictionary. A list of words compiled from BEST corpus is included in this package as a file 'BEST.dict'\n",
    "The standard dictionary used in this package has more then 65,000 entries including abbreviations and transliterations compiled from various sources. A dictionary of 8,700 proper names e.g. country names, organization names, location names, animal names, plant names, food names, …, such as 'อุซเบกิสถาน', 'สำนักเลขาธิการนายกรัฐมนตรี', 'วัดใหญ่สุวรรณาราม', 'หนอนเจาะลำต้นข้าวโพด', 'ปลาหมึกกระเทียมพริกไทย', are also added as a list of words in the system.\n",
    "For segmenting a specific domain text, a specialized dicionary can be used by adding more dictionary before segmenting texts. This can be done by calling read_thaidict(\"SPECIALIZED_DICT\"). Please note that the dictionary is a text file in \"iso-8859-11\" encoding. The format is one word per one line.\n",
    "'setence segment' or actually 'edu segment' is a process to break a paragraph into a chunk of discourse units, which usually are a clause. It is based on RandomForestClassifier model, which is trained on an edu-segmented corpus (approx. 7,000 edus) created and used in Nalinee's thesis (http://www.arts.chula.ac.th/~ling/thesis/2556MA-LING-Nalinee.pdf). Accuracy of the model is 97.8%. The reason behind using edu can be found in [Aroonmanakun, W. 2007. Thoughts on Word and Sentence Segmentation in Thai. In Proceedings of the Seventh Symposium on Natural Language Processing, Dec 13-15, 2007, Pattaya, Thailand. 85-90.] [Intasaw, N. and Aroonmanakun, W. 2013. Basic Principles for Segmenting Thai EDUs. in Proceedings of 27th Pacific Asia Conference on Language, Information, and Computation, pages 491-498, Nov 22-24, 2013, Taipei.]\n",
    "'grapheme to phoneme' (g2p), as well as IPA transcription (th2ipa) and Thai romanization (th2roman) is based on the hybrid approach presented in the paper \"A Unified Model of Thai Romanization and Word Segmentation\". The Thai Royal Institute guidline for Thai romanization can be downloaded from \"http://www.arts.chula.ac.th/~ling/tts/ThaiRoman.pdf\", or \"http://www.royin.go.th/?page_id=619\" [Aroonmanakun, W., and W. Rivepiboon. 2004. A Unified Model of Thai Word Segmentation and Romanization. In Proceedings of The 18th Pacific Asia Conference on Language, Information and Computation, Dec 8-10, 2004, Tokyo, Japan. 205-214.] (http://www.aclweb.org/anthology/Y04-1021)\n",
    "Remarks\n",
    "TNC Trigram data (TNC.3g) and TNC word2vec (TNCc5model.bin) can be downloaded from TNC website. http://www.arts.chula.ac.th/ling/tnc/searchtnc/\n",
    "Module \"spell_candidates\" is modified from Peter Norvig's Python codes at http://norvig.com/spell-correct.html\n",
    "Module \"w2v_compare_color\" is modified from http://chrisculy.net/lx/wordvectors/wvecs_visualization.html\n",
    "BEST corpus is the corpus released by NECTEC (https://www.nectec.or.th/corpus/)\n",
    "Universal POS tags are used in this project. For more information, please see http://universaldependencies.org/u/pos/index.html and http://www.arts.chula.ac.th/~ling/contents/File/UD%20Annotation%20for%20Thai.pdf\n",
    "pos_tag is based on PerceptronTagger in nltk.tag.perceptron. It is trained with TNC data manually pos-taged (approx. 148,000 words). Accuracy on pos tagging is 91.68%. NLTK PerceptronTagger is a port of the Textblob Averaged Perceptron Tagger, which can be found at https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n",
    "named entiy recognition module is a CRF model adapted from this tutorial (http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html). The model is trained with NER data used in Sasimimon's and Nutcha's theses (altogether 7,354 names in a corpus of 183,300 words). (http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip, http://pioneer.chula.ac.th/~awirote/ Data-Sasiwimon.zip) and NER data from AIforThai (https://aiforthai.in.th/) Only valid NE files from AIforThai are used. The total number of all NEs is 170,076. Accuracy of the model is reported below (88%).\n",
    "tag\tprecision\trecall\tf1-score\tsupport\n",
    "B-L\t0.56\t0.48\t0.52\t27105\n",
    "B-O\t0.72\t0.58\t0.64\t59613\n",
    "B-P\t0.82\t0.83\t0.83\t83358\n",
    "I-L\t0.52\t0.43\t0.47\t17859\n",
    "I-O\t0.67\t0.59\t0.63\t67396\n",
    "I-P\t0.85\t0.88\t0.86\t175069\n",
    "O\t0.92\t0.94\t0.93\t1032377\n",
    "accuracy\t \t \t0.88\t1462777\n",
    "macro avg\t0.72\t0.68\t0.70\t1462777\n",
    "weighted avg\t0.87\t0.88\t0.88\t1462777\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# tltk 와 다른 pythainlp 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pythainlp\n",
      "  Downloading pythainlp-2.2.2-py3-none-any.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tinydb>=3.0\n",
      "  Downloading tinydb-4.1.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (2.22.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from pythainlp) (0.9.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.22.0->pythainlp) (2.8)\n",
      "Installing collected packages: tinydb, pythainlp\n",
      "Successfully installed pythainlp-2.2.2 tinydb-4.1.1\n",
      "Collecting epitran\n",
      "  Downloading epitran-1.8-py2.py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 213 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting marisa-trie\n",
      "  Downloading marisa-trie-0.7.5.tar.gz (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from epitran) (46.0.0.post20200309)\n",
      "Collecting panphon>=0.16\n",
      "  Downloading panphon-0.17-py2.py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Using cached regex-2020.7.14.tar.gz (690 kB)\n",
      "Requirement already satisfied: unicodecsv in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from epitran) (0.14.1)\n",
      "Collecting munkres\n",
      "  Downloading munkres-1.1.2-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.5.3-cp37-cp37m-macosx_10_6_intel.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from panphon>=0.16->epitran) (5.3)\n",
      "Requirement already satisfied: numpy in /Users/iris/opt/anaconda3/lib/python3.7/site-packages (from panphon>=0.16->epitran) (1.18.1)\n",
      "Building wheels for collected packages: marisa-trie, regex\n",
      "  Building wheel for marisa-trie (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp37-cp37m-macosx_10_9_x86_64.whl size=178787 sha256=33ef33a140c1a1436f3a858546bb7210dae797e54f2e4f1b01b4bf179254bdd1\n",
      "  Stored in directory: /Users/iris/Library/Caches/pip/wheels/01/ac/46/c838fd1aee138a8fd92ae30b04020995182eb4d99dec1e12ea\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for regex: filename=regex-2020.7.14-cp37-cp37m-macosx_10_9_x86_64.whl size=286543 sha256=72255b5477298de76d611b4cfdae1bf6944cbcc5dbe1926324e17d22df2e53f9\n",
      "  Stored in directory: /Users/iris/Library/Caches/pip/wheels/5b/68/ce/2508b5a5afc13bd96566c62d3ffebea7b401477c2ead3e8cc0\n",
      "Successfully built marisa-trie regex\n",
      "Installing collected packages: marisa-trie, munkres, editdistance, regex, panphon, epitran\n",
      "Successfully installed editdistance-0.5.3 epitran-1.8 marisa-trie-0.7.5 munkres-1.1.2 panphon-0.17 regex-2020.7.14\n"
     ]
    }
   ],
   "source": [
    "# pip install required modules\n",
    "# uncomment if running from colab\n",
    "# see list of modules in `requirements` and `extras`\n",
    "# in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
    "# Code examples for basic functions in PyThaiNLP https://github.com/PyThaiNLP/pythainlp\n",
    "\n",
    "# https://www.thainlp.org/pythainlp/tutorials/notebooks/pythainlp_get_started.html\n",
    "\n",
    "# !pip install pythainlp\n",
    "# !pip install epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "\n",
    "pythainlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮฤฦะัาำิีึืุูเแโใไๅํ็่้๊๋ฯฺๆ์ํ๎๏๚๛๐๑๒๓๔๕๖๗๘๙฿'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.thai_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pythainlp.thai_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.thai_consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pythainlp.thai_consonants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กรรไกร', 'กระดาษ', 'ไข่', 'ค้อน', 'ผ้าไหม']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.util import collate\n",
    "\n",
    "thai_words = [\"ค้อน\", \"กระดาษ\", \"กรรไกร\", \"ไข่\", \"ผ้าไหม\"]\n",
    "collate(thai_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ผ้าไหม', 'ค้อน', 'ไข่', 'กระดาษ', 'กรรไกร']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate(thai_words, reverse=True) # sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Segmentation\n",
    "### At sentence, word, and sub-word levels.\n",
    "\n",
    "## Sentence\n",
    "### Default sentence tokenizer is \"crfcut\". Tokenization engine can be chosen ussing engine= parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default (crfcut):\n",
      "['พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว พุทธศักราช ๒๔๗๕ เป็นรัฐธรรมนูญฉบับชั่วคราว ', 'ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม ', 'ประกาศใช้เมื่อวันที่ 27 มิถุนายน พ.ศ. 2475 ', 'โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่ 24 มิถุนายน พ.ศ. 2475 โดยคณะราษฎร']\n",
      "\n",
      "whitespace+newline:\n",
      "['พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว', 'พุทธศักราช', '๒๔๗๕', 'เป็นรัฐธรรมนูญฉบับชั่วคราว', 'ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม', 'ประกาศใช้เมื่อวันที่', '27', 'มิถุนายน', 'พ.ศ.', '2475', 'โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่', '24', 'มิถุนายน', 'พ.ศ.', '2475', 'โดยคณะราษฎร']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp import sent_tokenize\n",
    "\n",
    "text = (\"พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว พุทธศักราช ๒๔๗๕ \"\n",
    "        \"เป็นรัฐธรรมนูญฉบับชั่วคราว ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม \"\n",
    "        \"ประกาศใช้เมื่อวันที่ 27 มิถุนายน พ.ศ. 2475 \"\n",
    "        \"โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่ 24 มิถุนายน พ.ศ. 2475 โดยคณะราษฎร\")\n",
    "\n",
    "print(\"default (crfcut):\")\n",
    "print(sent_tokenize(text))\n",
    "print(\"\\nwhitespace+newline:\")\n",
    "print(sent_tokenize(text, engine=\"whitespace+newline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.thainlp.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
